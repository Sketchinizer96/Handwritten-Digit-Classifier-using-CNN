import pandas as pd
import nltk
import re
from collections import Counter
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import json

# Step 1: Install NLTK resources
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

# Step 2: Load the Excel file
file_path = 'path_to_your_file.xlsx'  # Replace with your file path
df = pd.read_excel(file_path)

# Step 3: Initialize NLTK components
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Step 4: Define the text cleaning function
def clean_comment(comment):
    # Convert to lowercase
    comment = comment.lower()
    
    # Remove punctuation and numbers
    comment = re.sub(r'[^\w\s]', '', comment)
    comment = re.sub(r'\d+', '', comment)
    
    # Tokenize
    words = nltk.word_tokenize(comment)
    
    # Remove stopwords
    words = [word for word in words if word not in stop_words]
    
    # Lemmatize
    words = [lemmatizer.lemmatize(word) for word in words]
    
    return words

# Step 5: Apply the cleaning function to the comments
df['cleaned_comments'] = df['comments'].apply(clean_comment)

# Step 6: Group by tags and find the most common words
tagged_words = df.groupby('tags')['cleaned_comments'].apply(lambda x: [word for comment in x for word in comment])

# Count the most common words for each tag
common_words_by_tag = tagged_words.apply(lambda words: Counter(words).most_common())

# Step 7: Convert the output to a dictionary for JSON serialization
output_dict = {tag: dict(common_words) for tag, common_words in common_words_by_tag.items()}

# Step 8: Save the output as a JSON file
output_file_path = 'common_words_by_tag.json'  # Replace with your desired output file path
with open(output_file_path, 'w') as json_file:
    json.dump(output_dict, json_file, indent=4)

print(f"Output saved to {output_file_path}")


--------------

def clean_comment(comment):
    # Convert to lowercase
    comment = comment.lower()
    
    # Remove punctuation and numbers
    comment = re.sub(r'[^\w\s]', '', comment)
    comment = re.sub(r'\d+', '', comment)
    
    # Tokenize
    words = word_tokenize(comment)
    
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words]
    
    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]
    
    # Join words back into a single string
    cleaned_comment = ' '.join(words)
    
    return cleaned_comment

-------

import spacy
import contractions
from collections import Counter
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

nlp = spacy.load("en_core_web_sm")
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english') + ['product', 'service'])

def get_wordnet_pos(word):
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ, "N": wordnet.NOUN, "V": wordnet.VERB, "R": wordnet.ADV}
    return tag_dict.get(tag, wordnet.NOUN)

def clean_comment_advanced(comment):
    # Expand contractions
    comment = contractions.fix(comment)
    
    # Use SpaCy to tokenize and remove non-alphabetic characters
    doc = nlp(comment)
    words = [token.text.lower() for token in doc if token.is_alpha]
    
    # Remove stopwords and lemmatize
    words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words if word not in stop_words]
    
    # Filter out rare words (example: words that appear in less than 5 documents)
    word_counts = Counter(words)
    words = [word for word in words if word_counts[word] >= 5]
    
    return ' '.join(words)